---
title: "ollama + AnythingLLM 本地部署 deepseek-r1 和知识库"
date: 2025-1-28 12:00:00 +0800
categories: [tools]     
---

deepseek R1火了，而且开源，跟个风拉个本地模型玩玩。

# 本地部署大模型
开源大模型的获取途径其实挺多的，但为了方便，这里使用ollama来管理大模型。

大模型有一个比较重要的指标是参数数量，比如deepseek-r1就有1.5b、7b、8b、14b、32b、70b和671b这些版本的，b表示billion，671b就是6710亿参数，换算过来显存得4、500个gb才能跑动，本地部署显然不现实。笔者的笔记本上装的是4060，显存8gb，看了看测评决定玩7b的蒸馏版。

蒸馏版参数少了很多，能力肯定不如完整版的，所以只能是玩玩。deepseek的API很便宜，若觉得有价值可以换[API](https://cloud.siliconflow.cn/i/2Q91tfyg)用满血版的。

## 下载ollama
在[官网](https://ollama.com/)下载，下载源是github上的，可能需要*修炼轻功*才能正常下载。

目前(25.1.21)，ollama拉下来的模型默认安装在C盘，这对于笔者这样C盘红了的很不友好。可以按如下步骤更改模型安装位置：

- 正常安装；
- 找到Ollama安装目录，目前是安装在默认路径下的。去`~\AppData\Local\Programs`(~请自行展开)文件夹中找到Ollama文件夹，将其移动至别的盘，如`D:\program`；
- 将环境变量中Path里的`~\AppData\Local\Programs`改为`D:\program`；
- 添加一个`OLLAMA_MODELS`环境变量，值为存放大模型的目录，如`D:\ollama\models`。

完成后，在cmd(win+r输入cmd回车)中检查环境变量设置是否成功：
```bat
set
```

或者
```bat
echo %OLLAMA_MODELS%
```

## 安装大模型
使用ollama run安装大模型，可以指定参数数量，例如：
```bat
ollama run llama3.1:8b 
ollama run deepseek-r1:7b
```

使用``ollama list``列出本地大模型：
```
(base) PS D:> ollama list
NAME              ID              SIZE      MODIFIED
gemma2:9b         ff02c3702f32    5.4 GB    20 minutes ago
llama3.1:8b       46e0c10c039e    4.9 GB    38 minutes ago
deepseek-r1:7b    0a8c26691023    4.7 GB    59 minutes ago
```

运行命令和安装命令是一样的：
```bat
ollama run deepseek-r1:7b
```

本地跑的小模型能力有限，做不到完整版那样精通所有语言，所以如果想用中文对话的话，llama3.1很菜，gemma2也感觉一般，不如qwen和deepseek好。

如果需要API服务，需要命令行启动ollama：
```bat
ollama serve
```

# 设置知识库

为模型设置知识库可以提高正确回答的几率，目前有AnythingLLM和Dify等几个选择。稍微看了一些教程，觉得AnythingLLM比较简单，因此使用AnythingLLM来入门。

在[官网](https://anythingllm.com/desktop)下载AnythingLLM并安装。安装时要下载ollama lib，虽然这不是必要的，但如果你网络抽风，那就要以几kb的速度龟速下完才能安装完毕。所以安装时要么彻底关掉网络，要么*使用浮空术*确保你能顺利下载，不然会安很久很久。

安装完毕后不要急着打开AnythingLLM，先确保ollama在运行。使用命令
```bat
ollama serve
```

完成后打开AnythingLLM，新建一个工作区，名称随便填。可以看到工作区选项卡上有↑上传和⚙设置两个按钮，对应知识库设置和大模型设置。

## 上传文本
点击↑，弹出上传页面。可以上传文本文件，md或者PDF等格式。把上传的文件点击``move to workspace``加入工作区，点击``save and embed``开始转换成向量，这需要一段时间。转换完毕后，大模型就可以读取你提供的文本了。

## 选择大模型
点击⚙打开设置页面，**这里的选项设置完毕后需要点击底部的update workspace才能保存**。

在聊天设置里把LLM提供商改为ollama，此时应当自动加载出来ollama模型选项。选择deepseek-r1:7b或者一个顺眼的模型。下面还有一个关键的聊天模式设置，分为聊天和查询两种：
- 聊天：大模型在知识库找不到目标信息时会根据模型本身的知识库来回答，如果模型本身也找不到资料，则可能强行给你编一堆乱七八糟的东西出来。
- 查询：严格按照知识库里的知识回答，知识库里找不到的问题没法回答。

对于一些学术问题，给他喂几个综述进去再问可以显著提高回答质量，而且还能给出来引文：
![](https://pub-ec46b9a843f44891acf04d27fddf97e0.r2.dev/2025/02/20250204023703.png)

我这里随便喂的论文，回答可能有点偏向论文内容。但对比直接问得到的幽默回答，显然可以发现喂了论文的回答要强*N*<sub>A</sub>倍(顺便一提，这个回答里的Compound International System似乎是模型自己编的，至少我没搜到)：
```
(base) PS D:\> ollama run deepseek-r1:7b
>>> CIS(D)是什么
<think>

</think>

CIS(D) 是“Compound International System”的简称，指由联合国教科文组织（ UNESCO ）设立的 18 套国际课程体系之一。该体系旨在为不同国家的学生提供统一的教育标准和课程安排。

### 核心特点：
1. **语言与文化**：强调母语学习和文化背景理解。
2. **年龄分类**：按学生年龄设计课程，确保学习内容适合学生的认知水平。
3. **模块化结构**：课程分为多个模块，涵盖不同学科领域。
4. **国际通用性**：适用于全球多个国家的学生。

### 适用范围：
CIS(D) 适用于 6 至 17 岁的青少年，在全球多国教育体系中推广。学生通过学习该课程可获得统一的文凭，便于在不同国家转换学分或申请教育机会。

### 结构特点：
- **初级课程**：涵盖语言、数学等基础科目。
- **高级课程**：包括自然科学、社会科学等领域，设置详细的学习路径。
- **职业准备**：提供职业发展相关的课程内容。

CIS(D) 的目标是促进全球教育公平和可及性，为学生提供一致且高质量的教育体验。
```

## API
deepseek官网的API平台遭到了彼洋的攻击，只好暂时先用着第三方的。好多第三方都有免费额度，也提供免费的蒸馏模型。这里以[siliconflow](https://cloud.siliconflow.cn/i/2Q91tfy)为例。首先得注册。

> 用邀请码2Q91tfyg有免费额度领😸

登录之后在右侧API密钥面板选择新建API密钥，复制密钥备用。

- 把LLM提供商切换为Generic OpenAI；
- base url填``https://api.siliconflow.cn``；
- 密钥填上一步建立的那个；
- chat model name填deepseek-ai/DeepSeek-R1-Distill-Llama-8B或``https://cloud.siliconflow.cn/models``页面上任意一个你看着顺眼的模型。我这里试一下Qwen/Qwen2.5-7B-Instruct；
- 剩下的按默认填就好。

完成后，就可以问个问题测试了：
```
CIS(D)是一种计算分子激发态能量的方法。它基于CIS（Complete Active Space Second-Order Perturbation Theory）方法，但通过引入双激发项来改进。CIS(D)的主要特点包括：

1. **非迭代特性**：CIS(D)是非迭代的，这意味着它不需要像某些方法那样迭代收敛，从而使计算速度加快约10倍。

2. **效率**：通过使用分辨率身份（RI）近似，CIS(D)的单点能量计算可以高效地应用于非常大的系统。

3. **规模依赖性**：CIS(D)的计算成本通常是分子大小的五次方函数，类似于MP2方法对基态的计算。

4. **准确性**：虽然CIS(D)在某些情况下可能不如全方程方法准确，但它对于描述激发态势能面的平稳点具有良好的性能。

5. **发展与应用**：CIS(D)最初由Head-Gordon提出，自那时以来，该方法已经在多种分子上进行了验证和比较。

总的来说，CIS(D)是一种快速且有效的用于计算分子激发态能量的方法，具有广泛的适用性。
```
与调用本地模型类似，可以给出引用。Qwen2.5没有思考过程，不知道效果是不是会比同尺寸的r1差一些。

> PS: deepseek V3的API才2元/M token，对比Llama3.1-405B的21元/M token便宜太多了，r1也才16元/M token，怪不得显卡大地震呢。

# SillyTavern(酒馆)
[SillyTavern](https://sillytavern.pro/)是一个角色扮演专用AI平台，依赖[node.js](https://nodejs.org/en/download)运行。git下载：

```bat
git clone https://github.com/SillyTavern/SillyTavern -b release
```

双击Start.bat运行，会自动npm下载好依赖，然后打开网页。

上面的选项第二个是API，改成ollama，API地址填``http://127.0.0.1:11434/``

